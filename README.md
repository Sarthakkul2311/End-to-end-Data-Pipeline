# End-to-end-Data-Pipeline
I built an end-to-end Azure pipeline for streaming data. Raw data from Blob Storage was moved to Data Lake via Azure Data Factory's Copy Data. Databricks notebooks transformed the data and saved results back to Data Lake. The automated workflow streamlined ingestion, transformation, and storage for efficient processing.
